ing TensorFlow backend.
2018-12-01 19:05:53.451670: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-12-01 19:05:57.029272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-12-01 19:05:57.029677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties:
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1e.0
totalMemory: 11.17GiB freeMemory: 11.10GiB
2018-12-01 19:05:57.029732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-12-01 19:06:00.499769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-12-01 19:06:00.499817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0
2018-12-01 19:06:00.499832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N
2018-12-01 19:06:00.501020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10758 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)
2018-12-01 19:06:00.504247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-12-01 19:06:00.504282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-12-01 19:06:00.504293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0
2018-12-01 19:06:00.504299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N
2018-12-01 19:06:00.504516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/device:GPU:0 with10758 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)
[name: "/device:CPU:0"
device_type: "CPU"
memory_limit: 268435456
locality {
}
incarnation: 6788389801960579501
, name: "/device:XLA_GPU:0"
device_type: "XLA_GPU"
memory_limit: 17179869184
locality {
}
incarnation: 982813414645327354
physical_device_desc: "device: XLA_GPU device"
, name: "/device:XLA_CPU:0"
device_type: "XLA_CPU"
memory_limit: 17179869184
locality {
}
incarnation: 12803065805431152567
physical_device_desc: "device: XLA_CPU device"
, name: "/device:GPU:0"
device_type: "GPU"
memory_limit: 11281491559
locality {
  bus_id: 1
  links {
  }
}
incarnation: 6619647713671979917
physical_device_desc: "device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7"
]
              ImageId                                      EncodedPixels
count           81723                                              81723
unique          42556                                              81722
top     fd1de824c.jpg  43801 1 44567 4 45334 5 46100 8 46867 9 47636 ...
freq               15                                                  2
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, None, None, 3 0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, None, None, 6 1792        input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, None, None, 6 256         conv2d_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, None, None, 6 36928       batch_normalization_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, None, None, 6 256         conv2d_2[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, None, None, 6 0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, None, None, 1 73856       max_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, None, None, 1 512         conv2d_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, None, None, 1 147584      batch_normalization_3[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, None, None, 1 512         conv2d_4[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, None, None, 1 0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, None, None, 2 295168      max_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, None, None, 2 1024        conv2d_5[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, None, None, 2 590080      batch_normalization_5[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, None, None, 2 1024        conv2d_6[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, None, None, 2 0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, None, None, 5 1180160     max_pooling2d_3[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, None, None, 5 2048        conv2d_7[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, None, None, 5 2359808     batch_normalization_7[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, None, None, 5 2048        conv2d_8[0][0]
__________________________________________________________________________________________________
up_sampling2d_1 (UpSampling2D)  (None, None, None, 5 0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, None, None, 7 0           batch_normalization_6[0][0]
                                                                 up_sampling2d_1[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, None, None, 2 1769728     concatenate_1[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, None, None, 2 1024        conv2d_9[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, None, None, 2 590080      batch_normalization_9[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, None, None, 2 1024        conv2d_10[0][0]
__________________________________________________________________________________________________
up_sampling2d_2 (UpSampling2D)  (None, None, None, 2 0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, None, None, 3 0           batch_normalization_4[0][0]
                                                                 up_sampling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, None, None, 1 442496      concatenate_2[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, None, None, 1 512         conv2d_11[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, None, None, 1 147584      batch_normalization_11[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, None, None, 1 512         conv2d_12[0][0]
__________________________________________________________________________________________________
up_sampling2d_3 (UpSampling2D)  (None, None, None, 1 0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, None, None, 1 0           batch_normalization_2[0][0]
                                                                 up_sampling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, None, None, 6 110656      concatenate_3[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, None, None, 6 256         conv2d_13[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, None, None, 6 36928       batch_normalization_13[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, None, None, 6 256         conv2d_14[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, None, None, 1 65          batch_normalization_14[0][0]
==================================================================================================
Total params: 7,794,177
Trainable params: 7,788,545
Non-trainable params: 5,632
__________________________________________________________________________________________________
None
2018-12-01 19:06:03.448616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-12-01 19:06:03.448662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-12-01 19:06:03.448676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0
2018-12-01 19:06:03.448683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N
2018-12-01 19:06:03.448924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/device:GPU:0 with10758 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)
Epoch 1/100
100/100 [==============================] - 77s 768ms/step - loss: -0.0491 - val_loss: -0.0930

Epoch 00001: val_loss improved from inf to -0.09297, saving model to model.hdf5
Epoch 2/100
100/100 [==============================] - 66s 658ms/step - loss: -0.1096 - val_loss: -0.0181

Epoch 00002: val_loss did not improve from -0.09297
Epoch 3/100
100/100 [==============================] - 66s 662ms/step - loss: -0.3107 - val_loss: -0.0936

Epoch 00003: val_loss improved from -0.09297 to -0.09359, saving model to model.hdf5
Epoch 4/100
100/100 [==============================] - 66s 665ms/step - loss: -0.4208 - val_loss: -0.0070

Epoch 00004: val_loss did not improve from -0.09359
Epoch 5/100
100/100 [==============================] - 66s 663ms/step - loss: -0.4341 - val_loss: -0.3851

Epoch 00005: val_loss improved from -0.09359 to -0.38505, saving model to model.hdf5
Epoch 6/100
100/100 [==============================] - 67s 665ms/step - loss: -0.4732 - val_loss: -0.4851

Epoch 00006: val_loss improved from -0.38505 to -0.48512, saving model to model.hdf5
Epoch 7/100
100/100 [==============================] - 66s 665ms/step - loss: -0.5148 - val_loss: -0.4079

Epoch 00007: val_loss did not improve from -0.48512
Epoch 8/100
100/100 [==============================] - 66s 665ms/step - loss: -0.4802 - val_loss: -0.3532

Epoch 00008: val_loss did not improve from -0.48512
Epoch 9/100
100/100 [==============================] - 67s 665ms/step - loss: -0.4939 - val_loss: -0.1378

Epoch 00009: val_loss did not improve from -0.48512
Epoch 10/100
100/100 [==============================] - 67s 665ms/step - loss: -0.5131 - val_loss: -0.4635

Epoch 00010: val_loss did not improve from -0.48512
Epoch 11/100
100/100 [==============================] - 66s 665ms/step - loss: -0.5205 - val_loss: -0.4782

Epoch 00011: val_loss did not improve from -0.48512
Epoch 12/100
100/100 [==============================] - 67s 665ms/step - loss: -0.5606 - val_loss: -0.3119

Epoch 00012: val_loss did not improve from -0.48512
Epoch 13/100
100/100 [==============================] - 67s 665ms/step - loss: -0.5424 - val_loss: -0.4064

Epoch 00013: val_loss did not improve from -0.48512
Epoch 14/100
100/100 [==============================] - 67s 665ms/step - loss: -0.5697 - val_loss: -0.4263

Epoch 00014: val_loss did not improve from -0.48512
Epoch 15/100
100/100 [==============================] - 67s 665ms/step - loss: -0.5366 - val_loss: -0.4391

Epoch 00015: val_loss did not improve from -0.48512
Epoch 16/100
100/100 [==============================] - 66s 665ms/step - loss: -0.5561 - val_loss: -0.4472

Epoch 00016: val_loss did not improve from -0.48512
Epoch 00016: early stopping


=======================================================================================================================================
=======================================================================================================================================
=======================================================================================================================================
=======================================================================================================================================
=======================================================================================================================================
=======================================================================================================================================
=======================================================================================================================================
=======================================================================================================================================


poch 1/250
100/100 [==============================] - 75s 748ms/step - loss: -0.2566 - val_loss: -0.1732

Epoch 00001: val_loss improved from inf to -0.17319, saving model to model.hdf5
Epoch 2/250
100/100 [==============================] - 66s 660ms/step - loss: -0.2831 - val_loss: -0.1360

Epoch 00002: val_loss did not improve from -0.17319
Epoch 3/250
100/100 [==============================] - 66s 662ms/step - loss: -0.2315 - val_loss: -0.2172

Epoch 00003: val_loss improved from -0.17319 to -0.21723, saving model to model.hdf5
Epoch 4/250
100/100 [==============================] - 66s 663ms/step - loss: -0.2190 - val_loss: -0.1565

Epoch 00004: val_loss did not improve from -0.21723
Epoch 5/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2336 - val_loss: -0.1129

Epoch 00005: val_loss did not improve from -0.21723
Epoch 6/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2298 - val_loss: -0.1997

Epoch 00006: val_loss did not improve from -0.21723
Epoch 7/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2833 - val_loss: -0.2605

Epoch 00007: val_loss improved from -0.21723 to -0.26047, saving model to model.hdf5
Epoch 8/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2159 - val_loss: -0.2649

Epoch 00008: val_loss improved from -0.26047 to -0.26493, saving model to model.hdf5
Epoch 9/250
100/100 [==============================] - 66s 664ms/step - loss: -0.1809 - val_loss: -0.2548

Epoch 00009: val_loss did not improve from -0.26493
Epoch 10/250
100/100 [==============================] - 66s 664ms/step - loss: -0.3124 - val_loss: -0.2929

Epoch 00010: val_loss improved from -0.26493 to -0.29294, saving model to model.hdf5
Epoch 11/250
100/100 [==============================] - 66s 665ms/step - loss: -0.2027 - val_loss: -0.1990

Epoch 00011: val_loss did not improve from -0.29294
Epoch 12/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2976 - val_loss: -0.1409

Epoch 00012: val_loss did not improve from -0.29294
Epoch 13/250
100/100 [==============================] - 67s 666ms/step - loss: -0.2697 - val_loss: -0.2329

Epoch 00013: val_loss did not improve from -0.29294
Epoch 14/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2019 - val_loss: -0.2396

Epoch 00014: val_loss did not improve from -0.29294
Epoch 15/250
100/100 [==============================] - 66s 664ms/step - loss: -0.3105 - val_loss: -0.4785

Epoch 00015: val_loss improved from -0.29294 to -0.47850, saving model to model.hdf5
Epoch 16/250
100/100 [==============================] - 67s 666ms/step - loss: -0.2819 - val_loss: -0.2811

Epoch 00016: val_loss did not improve from -0.47850
Epoch 17/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2568 - val_loss: -0.2252

Epoch 00017: val_loss did not improve from -0.47850
Epoch 18/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2580 - val_loss: -0.2780

Epoch 00018: val_loss did not improve from -0.47850
Epoch 19/250
100/100 [==============================] - 66s 664ms/step - loss: -0.3142 - val_loss: -0.3830

Epoch 00019: val_loss did not improve from -0.47850
Epoch 20/250
100/100 [==============================] - 67s 665ms/step - loss: -0.2254 - val_loss: -0.1244

Epoch 00020: val_loss did not improve from -0.47850
Epoch 21/250
100/100 [==============================] - 66s 665ms/step - loss: -0.2911 - val_loss: -0.0793

Epoch 00021: val_loss did not improve from -0.47850
Epoch 22/250
100/100 [==============================] - 66s 665ms/step - loss: -0.2383 - val_loss: -0.3730

Epoch 00022: val_loss did not improve from -0.47850
Epoch 23/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2824 - val_loss: -0.2216

Epoch 00023: val_loss did not improve from -0.47850
Epoch 24/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2858 - val_loss: -0.3044

Epoch 00024: val_loss did not improve from -0.47850
Epoch 25/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2999 - val_loss: -0.4280

Epoch 00025: val_loss did not improve from -0.47850
Epoch 26/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2252 - val_loss: -0.4001

Epoch 00026: val_loss did not improve from -0.47850
Epoch 27/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2317 - val_loss: -0.2151

Epoch 00027: val_loss did not improve from -0.47850
Epoch 28/250
100/100 [==============================] - 66s 664ms/step - loss: -0.3331 - val_loss: -0.2325

Epoch 00028: val_loss did not improve from -0.47850
Epoch 29/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2558 - val_loss: -0.3131

Epoch 00029: val_loss did not improve from -0.47850
Epoch 30/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2977 - val_loss: -0.3475

Epoch 00030: val_loss did not improve from -0.47850
Epoch 31/250
100/100 [==============================] - 66s 664ms/step - loss: -0.3342 - val_loss: -0.0644

Epoch 00031: val_loss did not improve from -0.47850
Epoch 32/250
100/100 [==============================] - 66s 663ms/step - loss: -0.3211 - val_loss: -0.4439

Epoch 00032: val_loss did not improve from -0.47850
Epoch 33/250
100/100 [==============================] - 66s 664ms/step - loss: -0.3387 - val_loss: -0.2951

Epoch 00033: val_loss did not improve from -0.47850
Epoch 34/250
100/100 [==============================] - 66s 664ms/step - loss: -0.3237 - val_loss: -0.3674

Epoch 00034: val_loss did not improve from -0.47850
Epoch 35/250
100/100 [==============================] - 66s 665ms/step - loss: -0.3316 - val_loss: -0.3687

Epoch 00035: val_loss did not improve from -0.47850
Epoch 36/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2901 - val_loss: -0.3826

Epoch 00036: val_loss did not improve from -0.47850
Epoch 37/250
100/100 [==============================] - 66s 665ms/step - loss: -0.2945 - val_loss: -0.4333

Epoch 00037: val_loss did not improve from -0.47850
Epoch 38/250
100/100 [==============================] - 66s 664ms/step - loss: -0.3399 - val_loss: -0.3286

Epoch 00038: val_loss did not improve from -0.47850
Epoch 39/250
100/100 [==============================] - 66s 664ms/step - loss: -0.3225 - val_loss: -0.2963

Epoch 00039: val_loss did not improve from -0.47850
Epoch 40/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2865 - val_loss: -0.3325

Epoch 00040: val_loss did not improve from -0.47850
Epoch 41/250
100/100 [==============================] - 66s 665ms/step - loss: -0.2921 - val_loss: -0.3032

Epoch 00041: val_loss did not improve from -0.47850
Epoch 42/250
100/100 [==============================] - 66s 664ms/step - loss: -0.3462 - val_loss: -0.4959

Epoch 00042: val_loss improved from -0.47850 to -0.49585, saving model to model.hdf5
Epoch 43/250
100/100 [==============================] - 66s 664ms/step - loss: -0.3121 - val_loss: -0.4384

Epoch 00043: val_loss did not improve from -0.49585
Epoch 44/250
100/100 [==============================] - 66s 664ms/step - loss: -0.3058 - val_loss: -0.3358

Epoch 00044: val_loss did not improve from -0.49585
Epoch 45/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2943 - val_loss: -0.3807

Epoch 00045: val_loss did not improve from -0.49585
Epoch 46/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2827 - val_loss: -0.3737

Epoch 00046: val_loss did not improve from -0.49585
Epoch 47/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2971 - val_loss: -0.3152

Epoch 00047: val_loss did not improve from -0.49585
Epoch 48/250
100/100 [==============================] - 66s 664ms/step - loss: -0.3739 - val_loss: -0.4675

Epoch 00048: val_loss did not improve from -0.49585
Epoch 49/250
100/100 [==============================] - 66s 664ms/step - loss: -0.3162 - val_loss: -0.1470

Epoch 00049: val_loss did not improve from -0.49585
Epoch 50/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2767 - val_loss: -0.4221

Epoch 00050: val_loss did not improve from -0.49585
Epoch 51/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2947 - val_loss: -0.4142

Epoch 00051: val_loss did not improve from -0.49585
Epoch 52/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2900 - val_loss: -0.2997

Epoch 00052: val_loss did not improve from -0.49585
Epoch 53/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2956 - val_loss: -0.4386

Epoch 00053: val_loss did not improve from -0.49585
Epoch 54/250
100/100 [==============================] - 66s 664ms/step - loss: -0.3189 - val_loss: -0.3627

Epoch 00054: val_loss did not improve from -0.49585
Epoch 55/250
100/100 [==============================] - 66s 664ms/step - loss: -0.3417 - val_loss: -0.4012

Epoch 00055: val_loss did not improve from -0.49585
Epoch 56/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2963 - val_loss: -0.3872

Epoch 00056: val_loss did not improve from -0.49585
Epoch 57/250
100/100 [==============================] - 66s 664ms/step - loss: -0.3827 - val_loss: -0.2293

Epoch 00057: val_loss did not improve from -0.49585
Epoch 58/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2905 - val_loss: -0.3880

Epoch 00058: val_loss did not improve from -0.49585
Epoch 59/250
100/100 [==============================] - 66s 664ms/step - loss: -0.3116 - val_loss: -0.3128

Epoch 00059: val_loss did not improve from -0.49585
Epoch 60/250
100/100 [==============================] - 66s 663ms/step - loss: -0.3274 - val_loss: -0.4070

Epoch 00060: val_loss did not improve from -0.49585
Epoch 61/250
100/100 [==============================] - 66s 664ms/step - loss: -0.3502 - val_loss: -0.5525

Epoch 00061: val_loss improved from -0.49585 to -0.55246, saving model to model.hdf5
Epoch 62/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2939 - val_loss: -0.3452

Epoch 00062: val_loss did not improve from -0.55246
Epoch 63/250
100/100 [==============================] - 66s 663ms/step - loss: -0.3038 - val_loss: -0.4522

Epoch 00063: val_loss did not improve from -0.55246
Epoch 64/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2995 - val_loss: -0.2162

Epoch 00064: val_loss did not improve from -0.55246
Epoch 65/250
100/100 [==============================] - 66s 663ms/step - loss: -0.3308 - val_loss: -0.3673

Epoch 00065: val_loss did not improve from -0.55246
Epoch 66/250
100/100 [==============================] - 66s 663ms/step - loss: -0.2794 - val_loss: -0.4106

Epoch 00066: val_loss did not improve from -0.55246
Epoch 67/250
100/100 [==============================] - 66s 663ms/step - loss: -0.3530 - val_loss: -0.5210

Epoch 00067: val_loss did not improve from -0.55246
Epoch 68/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2972 - val_loss: -0.3586

Epoch 00068: val_loss did not improve from -0.55246
Epoch 69/250
100/100 [==============================] - 66s 663ms/step - loss: -0.3052 - val_loss: -0.3770

Epoch 00069: val_loss did not improve from -0.55246
Epoch 70/250
100/100 [==============================] - 66s 663ms/step - loss: -0.2873 - val_loss: -0.3470

Epoch 00070: val_loss did not improve from -0.55246
Epoch 71/250
100/100 [==============================] - 66s 664ms/step - loss: -0.3373 - val_loss: -0.4272

Epoch 00071: val_loss did not improve from -0.55246
Epoch 72/250
100/100 [==============================] - 66s 664ms/step - loss: -0.3262 - val_loss: -0.4123

Epoch 00072: val_loss did not improve from -0.55246
Epoch 73/250
100/100 [==============================] - 66s 664ms/step - loss: -0.3143 - val_loss: -0.3783

Epoch 00073: val_loss did not improve from -0.55246
Epoch 74/250
100/100 [==============================] - 66s 663ms/step - loss: -0.3317 - val_loss: -0.3676

Epoch 00074: val_loss did not improve from -0.55246
Epoch 75/250
100/100 [==============================] - 66s 663ms/step - loss: -0.3570 - val_loss: -0.4206

Epoch 00075: val_loss did not improve from -0.55246
Epoch 76/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2839 - val_loss: -0.4776

Epoch 00076: val_loss did not improve from -0.55246
Epoch 77/250
100/100 [==============================] - 66s 664ms/step - loss: -0.3217 - val_loss: -0.3849

Epoch 00077: val_loss did not improve from -0.55246
Epoch 78/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2980 - val_loss: -0.3238

Epoch 00078: val_loss did not improve from -0.55246
Epoch 79/250
100/100 [==============================] - 66s 663ms/step - loss: -0.3127 - val_loss: -0.4899

Epoch 00079: val_loss did not improve from -0.55246
Epoch 80/250
100/100 [==============================] - 66s 663ms/step - loss: -0.3636 - val_loss: -0.3457

Epoch 00080: val_loss did not improve from -0.55246
Epoch 81/250
100/100 [==============================] - 67s 665ms/step - loss: -0.3582 - val_loss: -0.4507

Epoch 00081: val_loss did not improve from -0.55246
Epoch 82/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2926 - val_loss: -0.3767

Epoch 00082: val_loss did not improve from -0.55246
Epoch 83/250
100/100 [==============================] - 66s 664ms/step - loss: -0.2608 - val_loss: -0.3282

Epoch 00083: val_loss did not improve from -0.55246
Epoch 84/250
100/100 [==============================] - 66s 664ms/step - loss: -0.3211 - val_loss: -0.2833

Epoch 00084: val_loss did not improve from -0.55246
Epoch 85/250
100/100 [==============================] - 67s 666ms/step - loss: -0.3600 - val_loss: -0.2437

Epoch 00085: val_loss did not improve from -0.55246
Epoch 86/250
100/100 [==============================] - 66s 664ms/step - loss: -0.3078 - val_loss: -0.3320

Epoch 00086: val_loss did not improve from -0.55246
Epoch 87/250
100/100 [==============================] - 67s 666ms/step - loss: -0.3297 - val_loss: -0.4665

Epoch 00087: val_loss did not improve from -0.55246
Epoch 88/250
100/100 [==============================] - 66s 663ms/step - loss: -0.3069 - val_loss: -0.4255

Epoch 00088: val_loss did not improve from -0.55246
Epoch 89/250
100/100 [==============================] - 66s 663ms/step - loss: -0.3225 - val_loss: -0.4608

Epoch 00089: val_loss did not improve from -0.55246
Epoch 90/250
100/100 [==============================] - 66s 663ms/step - loss: -0.3875 - val_loss: -0.3623

Epoch 00090: val_loss did not improve from -0.55246
Epoch 91/250
100/100 [==============================] - 66s 663ms/step - loss: -0.3665 - val_loss: -0.4552

Epoch 00091: val_loss did not improve from -0.55246
Epoch 00091: early stopping
[1]+  Killed                  pyth








poch 7/500
250/250 [==============================] - 415s 2s/step - loss: -0.5204 - val_loss: -0.3877

Epoch 00007: val_loss did not improve from -0.55221
Epoch 8/500
250/250 [==============================] - 415s 2s/step - loss: -0.5209 - val_loss: -0.4479

Epoch 00008: val_loss did not improve from -0.55221
Epoch 9/500
250/250 [==============================] - 415s 2s/step - loss: -0.5075 - val_loss: -0.2816

Epoch 00009: val_loss did not improve from -0.55221
Epoch 10/500
250/250 [==============================] - 415s 2s/step - loss: -0.5241 - val_loss: -0.3478

Epoch 00010: val_loss did not improve from -0.55221
Epoch 11/500
250/250 [==============================] - 415s 2s/step - loss: -0.5567 - val_loss: -0.5090

Epoch 00011: val_loss did not improve from -0.55221
Epoch 12/500
250/250 [==============================] - 415s 2s/step - loss: -0.5363 - val_loss: -0.4269

Epoch 00012: val_loss did not improve from -0.55221
Epoch 13/500
250/250 [==============================] - 415s 2s/step - loss: -0.5457 - val_loss: -0.4034

Epoch 00013: val_loss did not improve from -0.55221
Epoch 14/500
250/250 [==============================] - 415s 2s/step - loss: -0.5292 - val_loss: -0.4109

Epoch 00014: val_loss did not improve from -0.55221
Epoch 15/500
250/250 [==============================] - 414s 2s/step - loss: -0.5445 - val_loss: -0.4639

Epoch 00015: val_loss did not improve from -0.55221
Epoch 16/500
250/250 [==============================] - 414s 2s/step - loss: -0.5555 - val_loss: -0.5161

Epoch 00016: val_loss did not improve from -0.55221
Epoch 17/500
250/250 [==============================] - 415s 2s/step - loss: -0.5218 - val_loss: -0.5274

Epoch 00017: val_loss did not improve from -0.55221
Epoch 18/500
250/250 [==============================] - 414s 2s/step - loss: -0.5603 - val_loss: -0.5166

Epoch 00018: val_loss did not improve from -0.55221
Epoch 19/500
250/250 [==============================] - 414s 2s/step - loss: -0.5222 - val_loss: -0.4167

Epoch 00019: val_loss did not improve from -0.55221
Epoch 20/500
250/250 [==============================] - 414s 2s/step - loss: -0.5759 - val_loss: -0.5578

Epoch 00020: val_loss improved from -0.55221 to -0.55784, saving model to model.hdf5
Epoch 21/500
250/250 [==============================] - 414s 2s/step - loss: -0.5583 - val_loss: -0.4168

Epoch 00021: val_loss did not improve from -0.55784
Epoch 22/500
250/250 [==============================] - 415s 2s/step - loss: -0.5756 - val_loss: -0.4769

Epoch 00022: val_loss did not improve from -0.55784
Epoch 23/500
250/250 [==============================] - 414s 2s/step - loss: -0.5813 - val_loss: -0.4512

Epoch 00023: val_loss did not improve from -0.55784
Epoch 24/500
250/250 [==============================] - 414s 2s/step - loss: -0.5740 - val_loss: -0.4439

Epoch 00024: val_loss did not improve from -0.55784
Epoch 25/500
250/250 [==============================] - 414s 2s/step - loss: -0.5677 - val_loss: -0.4479

Epoch 00025: val_loss did not improve from -0.55784
Epoch 26/500
250/250 [==============================] - 414s 2s/step - loss: -0.5755 - val_loss: -0.5052

Epoch 00026: val_loss did not improve from -0.55784
Epoch 27/500
250/250 [==============================] - 414s 2s/step - loss: -0.6031 - val_loss: -0.4480

Epoch 00027: val_loss did not improve from -0.55784
Epoch 28/500
250/250 [==============================] - 415s 2s/step - loss: -0.5734 - val_loss: -0.5327

Epoch 00028: val_loss did not improve from -0.55784
Epoch 29/500
250/250 [==============================] - 415s 2s/step - loss: -0.5484 - val_loss: -0.4793

Epoch 00029: val_loss did not improve from -0.55784
Epoch 30/500
250/250 [==============================] - 413s 2s/step - loss: -0.5599 - val_loss: -0.4533

Epoch 00030: val_loss did not improve from -0.55784
Epoch 31/500
250/250 [==============================] - 415s 2s/step - loss: -0.5553 - val_loss: -0.4991

Epoch 00031: val_loss did not improve from -0.55784
Epoch 32/500
250/250 [==============================] - 415s 2s/step - loss: -0.5644 - val_loss: -0.5469

Epoch 00032: val_loss did not improve from -0.55784
Epoch 33/500
250/250 [==============================] - 415s 2s/step - loss: -0.5670 - val_loss: -0.3654

Epoch 00033: val_loss did not improve from -0.55784
Epoch 34/500
250/250 [==============================] - 413s 2s/step - loss: -0.5816 - val_loss: -0.3032

Epoch 00034: val_loss did not improve from -0.55784
Epoch 35/500
250/250 [==============================] - 413s 2s/step - loss: -0.5749 - val_loss: -0.4108

Epoch 00035: val_loss did not improve from -0.55784
Epoch 36/500
250/250 [==============================] - 414s 2s/step - loss: -0.5685 - val_loss: -0.5212

Epoch 00036: val_loss did not improve from -0.55784
Epoch 37/500
250/250 [==============================] - 414s 2s/step - loss: -0.5775 - val_loss: -0.5537

Epoch 00037: val_loss did not improve from -0.55784
Epoch 38/500
250/250 [==============================] - 413s 2s/step - loss: -0.5542 - val_loss: -0.5133

Epoch 00038: val_loss did not improve from -0.55784
Epoch 39/500
250/250 [==============================] - 413s 2s/step - loss: -0.5798 - val_loss: -0.5929

Epoch 00039: val_loss improved from -0.55784 to -0.59293, saving model to model.hdf5
Epoch 40/500
250/250 [==============================] - 414s 2s/step - loss: -0.5750 - val_loss: -0.5257

Epoch 00040: val_loss did not improve from -0.59293
Epoch 41/500
250/250 [==============================] - 414s 2s/step - loss: -0.5936 - val_loss: -0.6455

Epoch 00041: val_loss improved from -0.59293 to -0.64552, saving model to model.hdf5
Epoch 42/500
250/250 [==============================] - 414s 2s/step - loss: -0.6039 - val_loss: -0.6023

Epoch 00042: val_loss did not improve from -0.64552
Epoch 43/500
250/250 [==============================] - 413s 2s/step - loss: -0.5988 - val_loss: -0.5357

Epoch 00043: val_loss did not improve from -0.64552
Epoch 44/500
250/250 [==============================] - 414s 2s/step - loss: -0.5725 - val_loss: -0.5306

Epoch 00044: val_loss did not improve from -0.64552
Epoch 45/500
250/250 [==============================] - 413s 2s/step - loss: -0.5907 - val_loss: -0.4671

Epoch 00045: val_loss did not improve from -0.64552
Epoch 46/500
250/250 [==============================] - 413s 2s/step - loss: -0.6115 - val_loss: -0.5618

Epoch 00046: val_loss did not improve from -0.64552
Epoch 47/500
250/250 [==============================] - 413s 2s/step - loss: -0.5761 - val_loss: -0.4255

Epoch 00047: val_loss did not improve from -0.64552
Epoch 48/500
250/250 [==============================] - 414s 2s/step - loss: -0.5536 - val_loss: -0.5591

Epoch 00048: val_loss did not improve from -0.64552
Epoch 49/500
250/250 [==============================] - 415s 2s/step - loss: -0.6231 - val_loss: -0.4555

Epoch 00049: val_loss did not improve from -0.64552
Epoch 50/500
250/250 [==============================] - 413s 2s/step - loss: -0.5981 - val_loss: -0.5598

Epoch 00050: val_loss did not improve from -0.64552
Epoch 51/500
250/250 [==============================] - 415s 2s/step - loss: -0.5852 - val_loss: -0.5549

Epoch 00051: val_loss did not improve from -0.64552
Epoch 52/500
250/250 [==============================] - 414s 2s/step - loss: -0.6220 - val_loss: -0.5103












Epoch 137/500
50/250 [==============================] - 413s 2s/step - loss: -0.6912 - val_loss: -0.6754

Epoch 00137: val_loss did not improve from -0.75215
poch 138/500
250/250 [==============================] - 414s 2s/step - loss: -0.6773 - val_loss: -0.6763

Epoch 00138: val_loss did not improve from -0.75215
Epoch 139/500
250/250 [==============================] - 418s 2s/step - loss: -0.6971 - val_loss: -0.6435

Epoch 00139: val_loss did not improve from -0.75215
Epoch 140/500
250/250 [==============================] - 422s 2s/step - loss: -0.7012 - val_loss: -0.6050

Epoch 00140: val_loss did not improve from -0.75215
Epoch 141/500
250/250 [==============================] - 425s 2s/step - loss: -0.6897 - val_loss: -0.6920

Epoch 00141: val_loss did not improve from -0.75215
Epoch 142/500
250/250 [==============================] - 414s 2s/step - loss: -0.7075 - val_loss: -0.6790

Epoch 00142: val_loss did not improve from -0.75215
Epoch 143/500
250/250 [==============================] - 414s 2s/step - loss: -0.7040 - val_loss: -0.6928

Epoch 00143: val_loss did not improve from -0.75215
Epoch 144/500
250/250 [==============================] - 424s 2s/step - loss: -0.7054 - val_loss: -0.6487

Epoch 00144: val_loss did not improve from -0.75215
Epoch 145/500
250/250 [==============================] - 427s 2s/step - loss: -0.6983 - val_loss: -0.6777

Epoch 00145: val_loss did not improve from -0.75215
Epoch 146/500
250/250 [==============================] - 415s 2s/step - loss: -0.6973 - val_loss: -0.7485

Epoch 00146: val_loss did not improve from -0.75215
Epoch 147/500
250/250 [==============================] - 420s 2s/step - loss: -0.7089 - val_loss: -0.6732

Epoch 00147: val_loss did not improve from -0.75215
Epoch 148/500
250/250 [==============================] - 415s 2s/step - loss: -0.7083 - val_loss: -0.7173

Epoch 00148: val_loss did not improve from -0.75215
Epoch 149/500
250/250 [==============================] - 418s 2s/step - loss: -0.7127 - val_loss: -0.5495

Epoch 00149: val_loss did not improve from -0.75215
Epoch 150/500
250/250 [==============================] - 424s 2s/step - loss: -0.7011 - val_loss: -0.6440

Epoch 00150: val_loss did not improve from -0.75215
Epoch 151/500
250/250 [==============================] - 415s 2s/step - loss: -0.7118 - val_loss: -0.6422

Epoch 00151: val_loss did not improve from -0.75215
Epoch 152/500
250/250 [==============================] - 416s 2s/step - loss: -0.7009 - val_loss: -0.6815

Epoch 00152: val_loss did not improve from -0.75215
Epoch 153/500
250/250 [==============================] - 413s 2s/step - loss: -0.6998 - val_loss: -0.7057

