ing TensorFlow backend.
2018-12-01 19:05:53.451670: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-12-01 19:05:57.029272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-12-01 19:05:57.029677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties:
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1e.0
totalMemory: 11.17GiB freeMemory: 11.10GiB
2018-12-01 19:05:57.029732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-12-01 19:06:00.499769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-12-01 19:06:00.499817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0
2018-12-01 19:06:00.499832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N
2018-12-01 19:06:00.501020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10758 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)
2018-12-01 19:06:00.504247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-12-01 19:06:00.504282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-12-01 19:06:00.504293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0
2018-12-01 19:06:00.504299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N
2018-12-01 19:06:00.504516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/device:GPU:0 with10758 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)
[name: "/device:CPU:0"
device_type: "CPU"
memory_limit: 268435456
locality {
}
incarnation: 6788389801960579501
, name: "/device:XLA_GPU:0"
device_type: "XLA_GPU"
memory_limit: 17179869184
locality {
}
incarnation: 982813414645327354
physical_device_desc: "device: XLA_GPU device"
, name: "/device:XLA_CPU:0"
device_type: "XLA_CPU"
memory_limit: 17179869184
locality {
}
incarnation: 12803065805431152567
physical_device_desc: "device: XLA_CPU device"
, name: "/device:GPU:0"
device_type: "GPU"
memory_limit: 11281491559
locality {
  bus_id: 1
  links {
  }
}
incarnation: 6619647713671979917
physical_device_desc: "device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7"
]
              ImageId                                      EncodedPixels
count           81723                                              81723
unique          42556                                              81722
top     fd1de824c.jpg  43801 1 44567 4 45334 5 46100 8 46867 9 47636 ...
freq               15                                                  2
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, None, None, 3 0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, None, None, 6 1792        input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, None, None, 6 256         conv2d_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, None, None, 6 36928       batch_normalization_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, None, None, 6 256         conv2d_2[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, None, None, 6 0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, None, None, 1 73856       max_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, None, None, 1 512         conv2d_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, None, None, 1 147584      batch_normalization_3[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, None, None, 1 512         conv2d_4[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, None, None, 1 0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, None, None, 2 295168      max_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, None, None, 2 1024        conv2d_5[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, None, None, 2 590080      batch_normalization_5[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, None, None, 2 1024        conv2d_6[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, None, None, 2 0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, None, None, 5 1180160     max_pooling2d_3[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, None, None, 5 2048        conv2d_7[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, None, None, 5 2359808     batch_normalization_7[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, None, None, 5 2048        conv2d_8[0][0]
__________________________________________________________________________________________________
up_sampling2d_1 (UpSampling2D)  (None, None, None, 5 0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, None, None, 7 0           batch_normalization_6[0][0]
                                                                 up_sampling2d_1[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, None, None, 2 1769728     concatenate_1[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, None, None, 2 1024        conv2d_9[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, None, None, 2 590080      batch_normalization_9[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, None, None, 2 1024        conv2d_10[0][0]
__________________________________________________________________________________________________
up_sampling2d_2 (UpSampling2D)  (None, None, None, 2 0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, None, None, 3 0           batch_normalization_4[0][0]
                                                                 up_sampling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, None, None, 1 442496      concatenate_2[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, None, None, 1 512         conv2d_11[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, None, None, 1 147584      batch_normalization_11[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, None, None, 1 512         conv2d_12[0][0]
__________________________________________________________________________________________________
up_sampling2d_3 (UpSampling2D)  (None, None, None, 1 0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, None, None, 1 0           batch_normalization_2[0][0]
                                                                 up_sampling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, None, None, 6 110656      concatenate_3[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, None, None, 6 256         conv2d_13[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, None, None, 6 36928       batch_normalization_13[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, None, None, 6 256         conv2d_14[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, None, None, 1 65          batch_normalization_14[0][0]
==================================================================================================
Total params: 7,794,177
Trainable params: 7,788,545
Non-trainable params: 5,632
__________________________________________________________________________________________________
None
Epoch 00001: val_loss improved from inf to -0.02015, saving model to model.hdf5
Epoch 2/100
100/100 [==============================] - 66s 658ms/step - loss: -0.0414 - val_loss: -0.0334

Epoch 00002: val_loss improved from -0.02015 to -0.03343, saving model to model.hdf5
Epoch 3/100
100/100 [==============================] - 66s 662ms/step - loss: -0.0831 - val_loss: -0.0173

Epoch 00003: val_loss did not improve from -0.03343
Epoch 4/100
100/100 [==============================] - 66s 663ms/step - loss: -0.0811 - val_loss: -0.0186

Epoch 00004: val_loss did not improve from -0.03343
Epoch 5/100
100/100 [==============================] - 66s 664ms/step - loss: -0.1131 - val_loss: -0.0763

Epoch 00005: val_loss improved from -0.03343 to -0.07633, saving model to model.hdf5
Epoch 6/100
100/100 [==============================] - 66s 663ms/step - loss: -0.1371 - val_loss: -0.0986

Epoch 00006: val_loss improved from -0.07633 to -0.09861, saving model to model.hdf5
Epoch 7/100
100/100 [==============================] - 66s 663ms/step - loss: -0.1841 - val_loss: -0.0805

Epoch 00007: val_loss did not improve from -0.09861
Epoch 8/100
100/100 [==============================] - 66s 664ms/step - loss: -0.1489 - val_loss: -0.0162

Epoch 00008: val_loss did not improve from -0.09861
Epoch 9/100
100/100 [==============================] - 66s 664ms/step - loss: -0.1638 - val_loss: -0.1482

Epoch 00009: val_loss improved from -0.09861 to -0.14820, saving model to model.hdf5
Epoch 10/100
100/100 [==============================] - 66s 664ms/step - loss: -0.1684 - val_loss: -0.2147

Epoch 00010: val_loss improved from -0.14820 to -0.21467, saving model to model.hdf5
Epoch 11/100
100/100 [==============================] - 66s 664ms/step - loss: -0.2170 - val_loss: -0.1387

Epoch 00011: val_loss did not improve from -0.21467
Epoch 12/100
100/100 [==============================] - 66s 665ms/step - loss: -0.1971 - val_loss: -0.2608

Epoch 00012: val_loss improved from -0.21467 to -0.26081, saving model to model.hdf5
Epoch 13/100
100/100 [==============================] - 66s 664ms/step - loss: -0.1860 - val_loss: -0.2354

Epoch 00013: val_loss did not improve from -0.26081
Epoch 14/100
100/100 [==============================] - 66s 664ms/step - loss: -0.2211 - val_loss: -0.1413

Epoch 00014: val_loss did not improve from -0.26081
Epoch 15/100
100/100 [==============================] - 66s 664ms/step - loss: -0.3045 - val_loss: -0.1819

Epoch 00015: val_loss did not improve from -0.26081
Epoch 16/100
100/100 [==============================] - 66s 665ms/step - loss: -0.2308 - val_loss: -0.2170

Epoch 00016: val_loss did not improve from -0.26081
Epoch 17/100
100/100 [==============================] - 66s 664ms/step - loss: -0.2221 - val_loss: -0.2915

Epoch 00017: val_loss improved from -0.26081 to -0.29148, saving model to model.hdf5
Epoch 18/100
100/100 [==============================] - 66s 664ms/step - loss: -0.2202 - val_loss: -0.1480

Epoch 00018: val_loss did not improve from -0.29148
Epoch 19/100
100/100 [==============================] - 66s 664ms/step - loss: -0.2461 - val_loss: -0.2515

Epoch 00019: val_loss did not improve from -0.29148
Epoch 20/100
100/100 [==============================] - 66s 664ms/step - loss: -0.1867 - val_loss: -0.1781

Epoch 00020: val_loss did not improve from -0.29148
Epoch 21/100
100/100 [==============================] - 66s 664ms/step - loss: -0.2649 - val_loss: -0.1867

Epoch 00021: val_loss did not improve from -0.29148
Epoch 22/100
100/100 [==============================] - 66s 664ms/step - loss: -0.2501 - val_loss: -0.1679

Epoch 00022: val_loss did not improve from -0.29148
Epoch 23/100
100/100 [==============================] - 66s 664ms/step - loss: -0.2367 - val_loss: -0.3055

Epoch 00023: val_loss improved from -0.29148 to -0.30554, saving model to model.hdf5
Epoch 24/100
100/100 [==============================] - 66s 664ms/step - loss: -0.2962 - val_loss: -0.2853

Epoch 00024: val_loss did not improve from -0.30554
Epoch 25/100
100/100 [==============================] - 66s 665ms/step - loss: -0.2399 - val_loss: -0.0680

Epoch 00025: val_loss did not improve from -0.30554
Epoch 26/100
100/100 [==============================] - 66s 664ms/step - loss: -0.2603 - val_loss: -0.3429

Epoch 00026: val_loss improved from -0.30554 to -0.34288, saving model to model.hdf5
Epoch 27/100
100/100 [==============================] - 66s 664ms/step - loss: -0.2505 - val_loss: -0.2061

Epoch 00027: val_loss did not improve from -0.34288
Epoch 28/100
100/100 [==============================] - 66s 664ms/step - loss: -0.2824 - val_loss: -0.1907

Epoch 00028: val_loss did not improve from -0.34288
Epoch 29/100
100/100 [==============================] - 66s 664ms/step - loss: -0.2469 - val_loss: -0.3738

Epoch 00029: val_loss improved from -0.34288 to -0.37384, saving model to model.hdf5
Epoch 30/100
100/100 [==============================] - 66s 664ms/step - loss: -0.2778 - val_loss: -0.3477

Epoch 00030: val_loss did not improve from -0.37384
Epoch 31/100
100/100 [==============================] - 66s 664ms/step - loss: -0.2108 - val_loss: -0.2672

Epoch 00031: val_loss did not improve from -0.37384
Epoch 32/100
100/100 [==============================] - 66s 664ms/step - loss: -0.2708 - val_loss: -0.2672

Epoch 00032: val_loss did not improve from -0.37384
Epoch 33/100
100/100 [==============================] - 66s 665ms/step - loss: -0.3023 - val_loss: -0.1723

Epoch 00033: val_loss did not improve from -0.37384
Epoch 34/100
100/100 [==============================] - 66s 664ms/step - loss: -0.2535 - val_loss: -0.2029

Epoch 00034: val_loss did not improve from -0.37384
Epoch 35/100
100/100 [==============================] - 66s 664ms/step - loss: -0.3024 - val_loss: -0.3251

Epoch 00035: val_loss did not improve from -0.37384
Epoch 36/100
100/100 [==============================] - 66s 664ms/step - loss: -0.2304 - val_loss: -0.1962

Epoch 00036: val_loss did not improve from -0.37384
Epoch 37/100
100/100 [==============================] - 67s 665ms/step - loss: -0.2619 - val_loss: -0.1866

Epoch 00037: val_loss did not improve from -0.37384
Epoch 38/100
100/100 [==============================] - 66s 665ms/step - loss: -0.2673 - val_loss: -0.2551

Epoch 00038: val_loss did not improve from -0.37384
Epoch 39/100
100/100 [==============================] - 66s 664ms/step - loss: -0.3251 - val_loss: -0.3112

Epoch 00039: val_loss did not improve from -0.37384
Epoch 40/100
100/100 [==============================] - 66s 664ms/step - loss: -0.2875 - val_loss: -0.2336

Epoch 00040: val_loss did not improve from -0.37384
Epoch 41/100
100/100 [==============================] - 66s 664ms/step - loss: -0.2568 - val_loss: -0.3642

Epoch 00041: val_loss did not improve from -0.37384
Epoch 42/100
100/100 [==============================] - 66s 663ms/step - loss: -0.2694 - val_loss: -0.4582

Epoch 00042: val_loss improved from -0.37384 to -0.45817, saving model to model.hdf5
Epoch 43/100
100/100 [==============================] - 66s 663ms/step - loss: -0.1942 - val_loss: -0.4766

Epoch 00043: val_loss improved from -0.45817 to -0.47663, saving model to model.hdf5
Epoch 44/100
100/100 [==============================] - 66s 663ms/step - loss: -0.3141 - val_loss: -0.2198

Epoch 00044: val_loss did not improve from -0.47663
Epoch 45/100
100/100 [==============================] - 66s 664ms/step - loss: -0.2464 - val_loss: -0.2712

Epoch 00045: val_loss did not improve from -0.47663
Epoch 46/100
100/100 [==============================] - 66s 664ms/step - loss: -0.2509 - val_loss: -0.3225

Epoch 00046: val_loss did not improve from -0.47663
Epoch 47/100
100/100 [==============================] - 66s 664ms/step - loss: -0.3230 - val_loss: -0.2138

Epoch 00047: val_loss did not improve from -0.47663
Epoch 48/100
100/100 [==============================] - 66s 664ms/step - loss: -0.2433 - val_loss: -0.3602

Epoch 00048: val_loss did not improve from -0.47663
Epoch 49/100
100/100 [==============================] - 66s 664ms/step - loss: -0.2536 - val_loss: -0.4028

Epoch 00049: val_loss did not improve from -0.47663
Epoch 50/100
100/100 [==============================] - 66s 664ms/step - loss: -0.2892 - val_loss: -0.2968

Epoch 00050: val_loss did not improve from -0.47663
Epoch 51/100
100/100 [==============================] - 66s 664ms/step - loss: -0.3013 - val_loss: -0.3486

Epoch 00051: val_loss did not improve from -0.47663
Epoch 52/100
100/100 [==============================] - 66s 664ms/step - loss: -0.3294 - val_loss: -0.2875

Epoch 00052: val_loss did not improve from -0.47663
Epoch 53/100
100/100 [==============================] - 66s 664ms/step - loss: -0.3293 - val_loss: -0.4375

Epoch 00053: val_loss did not improve from -0.47663
Epoch 54/100
100/100 [==============================] - 66s 664ms/step - loss: -0.2689 - val_loss: -0.3097

Epoch 00054: val_loss did not improve from -0.47663
Epoch 55/100
100/100 [==============================] - 66s 664ms/step - loss: -0.3269 - val_loss: -0.2567

Epoch 00055: val_loss did not improve from -0.47663
Epoch 56/100
100/100 [==============================] - 66s 664ms/step - loss: -0.3199 - val_loss: -0.2163

Epoch 00056: val_loss did not improve from -0.47663
Epoch 57/100
100/100 [==============================] - 66s 664ms/step - loss: -0.2260 - val_loss: -0.3268

Epoch 00057: val_loss did not improve from -0.47663
Epoch 58/100
100/100 [==============================] - 66s 664ms/step - loss: -0.3018 - val_loss: -0.3712

Epoch 00058: val_loss did not improve from -0.47663
Epoch 00058: early stopping
[ec2-user@ip-172-31-32-136 Scr
