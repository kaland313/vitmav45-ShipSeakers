[name: "/device:CPU:0"
device_type: "CPU"
memory_limit: 268435456
locality {
}
incarnation: 17637327195081446403
, name: "/device:XLA_GPU:0"
device_type: "XLA_GPU"
memory_limit: 17179869184
locality {
}
incarnation: 2280677625495443260
physical_device_desc: "device: XLA_GPU device"
, name: "/device:XLA_CPU:0"
device_type: "XLA_CPU"
memory_limit: 17179869184
locality {
}
incarnation: 10080509075516032007
physical_device_desc: "device: XLA_CPU device"
, name: "/device:GPU:0"
device_type: "GPU"
memory_limit: 11281553818
locality {
  bus_id: 1
  links {
  }
}
incarnation: 3840738368847657882
physical_device_desc: "device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7"
]

__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 768, 768, 3)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 768, 768, 32) 896         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 768, 768, 32) 128         conv2d_1[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 768, 768, 32) 9248        batch_normalization_1[0][0]      
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 768, 768, 32) 128         conv2d_2[0][0]                   
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 384, 384, 32) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 384, 384, 64) 18496       max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 384, 384, 64) 256         conv2d_3[0][0]                   
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 384, 384, 64) 36928       batch_normalization_3[0][0]      
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 384, 384, 64) 256         conv2d_4[0][0]                   
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 192, 192, 64) 0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 192, 192, 128 73856       max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 192, 192, 128 512         conv2d_5[0][0]                   
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 192, 192, 128 147584      batch_normalization_5[0][0]      
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 192, 192, 128 512         conv2d_6[0][0]                   
__________________________________________________________________________________________________
up_sampling2d_1 (UpSampling2D)  (None, 384, 384, 128 0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 384, 384, 192 0           batch_normalization_4[0][0]      
                                                                 up_sampling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 384, 384, 64) 110656      concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 384, 384, 64) 256         conv2d_7[0][0]                   
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 384, 384, 64) 36928       batch_normalization_7[0][0]      
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 384, 384, 64) 256         conv2d_8[0][0]                   
__________________________________________________________________________________________________
up_sampling2d_2 (UpSampling2D)  (None, 768, 768, 64) 0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 768, 768, 96) 0           batch_normalization_2[0][0]      
                                                                 up_sampling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 768, 768, 32) 27680       concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 768, 768, 32) 128         conv2d_9[0][0]                   
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 768, 768, 32) 9248        batch_normalization_9[0][0]      
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 768, 768, 32) 128         conv2d_10[0][0]                  
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 768, 768, 1)  33          batch_normalization_10[0][0]     
==================================================================================================
Total params: 474,113
Trainable params: 472,833
Non-trainable params: 1,280
__________________________________________________________________________________________________


epoch: 0	loss: -0.300364015115	val_loss: -0.129578726194	lr: 0.001
epoch: 1	loss: -0.327489598632	val_loss: -0.146529755386	lr: 0.001
epoch: 2	loss: -0.276057328915	val_loss: -0.276322260627	lr: 0.001
epoch: 3	loss: -0.301171096516	val_loss: -0.111248222057	lr: 0.001
epoch: 4	loss: -0.297820472206	val_loss: -0.364087838922	lr: 0.001
epoch: 5	loss: -0.347043108571	val_loss: -0.344825744244	lr: 0.001
epoch: 6	loss: -0.288811493672	val_loss: -0.284805251266	lr: 0.001
epoch: 7	loss: -0.315972487365	val_loss: -0.246999401908	lr: 0.001
epoch: 8	loss: -0.294058100584	val_loss: -0.372253551342	lr: 0.001
epoch: 9	loss: -0.283987252903	val_loss: -0.301373417311	lr: 0.001
epoch: 10	loss: -0.341983016955	val_loss: -0.263274180693	lr: 0.001
epoch: 11	loss: -0.317308563718	val_loss: -0.38228671557	lr: 0.001
epoch: 12	loss: -0.337796785877	val_loss: -0.300532381018	lr: 0.001
epoch: 13	loss: -0.355496389313	val_loss: -0.329560843672	lr: 0.001
epoch: 14	loss: -0.317420101362	val_loss: -0.334139401049	lr: 0.001
epoch: 15	loss: -0.359830838539	val_loss: -0.210389494271	lr: 0.001
epoch: 16	loss: -0.353206486831	val_loss: -0.271382327263	lr: 0.0005
epoch: 17	loss: -0.314980583652	val_loss: -0.255704460773	lr: 0.0005
epoch: 18	loss: -0.330463121658	val_loss: -0.274375549484	lr: 0.0005
epoch: 19	loss: -0.399494878328	val_loss: -0.28690407032	lr: 0.0005
epoch: 20	loss: -0.354646511157	val_loss: -0.412540519796	lr: 0.00025
epoch: 21	loss: -0.345055248841	val_loss: -0.295249959687	lr: 0.00025
epoch: 22	loss: -0.385920382419	val_loss: -0.400967942784	lr: 0.00025
epoch: 23	loss: -0.344449720329	val_loss: -0.401260551113	lr: 0.00025
epoch: 24	loss: -0.369119778938	val_loss: -0.275045689882	lr: 0.00025
epoch: 25	loss: -0.367620466557	val_loss: -0.404416089552	lr: 0.000125
epoch: 26	loss: -0.354916002779	val_loss: -0.452306452274	lr: 0.000125
epoch: 27	loss: -0.356730091398	val_loss: -0.35514671182	lr: 0.000125
epoch: 28	loss: -0.414056327581	val_loss: -0.322135248017	lr: 0.000125
epoch: 29	loss: -0.392756475971	val_loss: -0.297498163895	lr: 0.000125
epoch: 30	loss: -0.361299789248	val_loss: -0.383674930891	lr: 0.000125
epoch: 31	loss: -0.363871518247	val_loss: -0.448427250525	lr: 6.25e-05
epoch: 32	loss: -0.385593832074	val_loss: -0.3686963293	lr: 6.25e-05
epoch: 33	loss: -0.444553515547	val_loss: -0.402340559503	lr: 6.25e-05
epoch: 34	loss: -0.368390036529	val_loss: -0.379513865258	lr: 6.25e-05
epoch: 35	loss: -0.351280950773	val_loss: -0.390564083263	lr: 3.125e-05
epoch: 36	loss: -0.362820312675	val_loss: -0.329315008222	lr: 3.125e-05
epoch: 37	loss: -0.358930712869	val_loss: -0.460981166773	lr: 3.125e-05
epoch: 38	loss: -0.356116254303	val_loss: -0.423519867817	lr: 3.125e-05
epoch: 39	loss: -0.342257060265	val_loss: -0.448727878244	lr: 3.125e-05
epoch: 40	loss: -0.432899632907	val_loss: -0.293564838944	lr: 3.125e-05
epoch: 41	loss: -0.43343879712	val_loss: -0.287505098921	lr: 3.125e-05
epoch: 42	loss: -0.358614060766	val_loss: -0.358943302161	lr: 1.5625e-05
epoch: 43	loss: -0.366366575989	val_loss: -0.409320459362	lr: 1.5625e-05
epoch: 44	loss: -0.375844942686	val_loss: -0.420725621292	lr: 1.5625e-05
epoch: 45	loss: -0.390599238206	val_loss: -0.379720556579	lr: 1.5625e-05
epoch: 46	loss: -0.395244212874	val_loss: -0.335406553916	lr: 7.8125e-06
epoch: 47	loss: -0.401379804173	val_loss: -0.362631227672	lr: 7.8125e-06
epoch: 48	loss: -0.404321670943	val_loss: -0.370730676726	lr: 7.8125e-06
epoch: 49	loss: -0.438305641046	val_loss: -0.437365972917	lr: 7.8125e-06
Training stopped here with early stopping.
Training restarted here without the learning rate reduction on plateu callback.
Epoch 1/1000
100/100 [==============================] - 162s 2s/step - loss: -0.3765 - val_loss: -0.2405

Epoch 00001: val_loss improved from inf to -0.24049, saving model to model.hdf5
Epoch 2/1000
100/100 [==============================] - 151s 2s/step - loss: -0.3558 - val_loss: -0.2573

Epoch 00002: val_loss improved from -0.24049 to -0.25731, saving model to model.hdf5
Epoch 3/1000
100/100 [==============================] - 152s 2s/step - loss: -0.3361 - val_loss: -0.1417

Epoch 00003: val_loss did not improve from -0.25731
Epoch 4/1000
100/100 [==============================] - 150s 2s/step - loss: -0.3627 - val_loss: -0.1956

Epoch 00004: val_loss did not improve from -0.25731
Epoch 5/1000
100/100 [==============================] - 150s 1s/step - loss: -0.2942 - val_loss: -0.3818

Epoch 00005: val_loss improved from -0.25731 to -0.38177, saving model to model.hdf5
Epoch 6/1000
100/100 [==============================] - 150s 1s/step - loss: -0.3055 - val_loss: -0.2269

Epoch 00006: val_loss did not improve from -0.38177
Epoch 7/1000
100/100 [==============================] - 150s 1s/step - loss: -0.3059 - val_loss: -0.3999

Epoch 00007: val_loss improved from -0.38177 to -0.39993, saving model to model.hdf5
Epoch 8/1000
100/100 [==============================] - 150s 2s/step - loss: -0.3830 - val_loss: -0.3495

Epoch 00008: val_loss did not improve from -0.39993
Epoch 9/1000
100/100 [==============================] - 152s 2s/step - loss: -0.3215 - val_loss: -0.2492

Epoch 00009: val_loss did not improve from -0.39993
Epoch 10/1000
100/100 [==============================] - 152s 2s/step - loss: -0.2953 - val_loss: -0.4449

Epoch 00010: val_loss improved from -0.39993 to -0.44489, saving model to model.hdf5
Epoch 11/1000
100/100 [==============================] - 151s 2s/step - loss: -0.3906 - val_loss: -0.2371

Epoch 00011: val_loss did not improve from -0.44489
Epoch 12/1000
100/100 [==============================] - 150s 1s/step - loss: -0.3991 - val_loss: -0.3227

Epoch 00012: val_loss did not improve from -0.44489
Epoch 13/1000
100/100 [==============================] - 150s 2s/step - loss: -0.3184 - val_loss: -0.2944

Epoch 00013: val_loss did not improve from -0.44489
Epoch 14/1000
100/100 [==============================] - 151s 2s/step - loss: -0.3343 - val_loss: -0.2926

Epoch 00014: val_loss did not improve from -0.44489
Epoch 15/1000
100/100 [==============================] - 151s 2s/step - loss: -0.3378 - val_loss: -0.2760

Epoch 00015: val_loss did not improve from -0.44489
Epoch 16/1000
100/100 [==============================] - 151s 2s/step - loss: -0.3038 - val_loss: -0.2444

Epoch 00016: val_loss did not improve from -0.44489
Epoch 17/1000
100/100 [==============================] - 151s 2s/step - loss: -0.3476 - val_loss: -0.3277

Epoch 00017: val_loss did not improve from -0.44489
Epoch 18/1000
100/100 [==============================] - 152s 2s/step - loss: -0.3706 - val_loss: -0.3034

Epoch 00018: val_loss did not improve from -0.44489

Unfortuanetly the model couldn't improve past -0.46 validation accuracy
