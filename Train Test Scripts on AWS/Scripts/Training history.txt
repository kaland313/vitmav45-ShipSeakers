[name: "/device:CPU:0"
device_type: "CPU"
memory_limit: 268435456
locality {
}
incarnation: 12233880830783111372
, name: "/device:XLA_GPU:0"
device_type: "XLA_GPU"
memory_limit: 17179869184
locality {
}
incarnation: 15424021723293493490
physical_device_desc: "device: XLA_GPU device"
, name: "/device:XLA_CPU:0"
device_type: "XLA_CPU"
memory_limit: 17179869184
locality {
}
incarnation: 13660381885664538751
physical_device_desc: "device: XLA_CPU device"
, name: "/device:GPU:0"
device_type: "GPU"
memory_limit: 11281491559
locality {
  bus_id: 1
  links {
  }
}
incarnation: 11283336892123951957
physical_device_desc: "device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7"
]
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 768, 768, 3)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 768, 768, 32) 896         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 768, 768, 32) 128         conv2d_1[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 768, 768, 32) 9248        batch_normalization_1[0][0]      
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 768, 768, 32) 128         conv2d_2[0][0]                   
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 384, 384, 32) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 384, 384, 64) 18496       max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 384, 384, 64) 256         conv2d_3[0][0]                   
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 384, 384, 64) 36928       batch_normalization_3[0][0]      
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 384, 384, 64) 256         conv2d_4[0][0]                   
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 192, 192, 64) 0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 192, 192, 128 73856       max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 192, 192, 128 512         conv2d_5[0][0]                   
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 192, 192, 128 147584      batch_normalization_5[0][0]      
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 192, 192, 128 512         conv2d_6[0][0]                   
__________________________________________________________________________________________________
up_sampling2d_1 (UpSampling2D)  (None, 384, 384, 128 0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 384, 384, 192 0           batch_normalization_4[0][0]      
                                                                 up_sampling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 384, 384, 64) 110656      concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 384, 384, 64) 256         conv2d_7[0][0]                   
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 384, 384, 64) 36928       batch_normalization_7[0][0]      
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 384, 384, 64) 256         conv2d_8[0][0]                   
__________________________________________________________________________________________________
up_sampling2d_2 (UpSampling2D)  (None, 768, 768, 64) 0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 768, 768, 96) 0           batch_normalization_2[0][0]      
                                                                 up_sampling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 768, 768, 32) 27680       concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 768, 768, 32) 128         conv2d_9[0][0]                   
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 768, 768, 32) 9248        batch_normalization_9[0][0]      
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 768, 768, 32) 128         conv2d_10[0][0]                  
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 768, 768, 1)  33          batch_normalization_10[0][0]     
==================================================================================================
Total params: 474,113
Trainable params: 472,833
Non-trainable params: 1,280
__________________________________________________________________________________________________
None
Epoch 1/1000
2018-11-16 22:58:50.544100: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.53GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-11-16 22:58:51.075606: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.53GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
100/100 [==============================] - 164s 2s/step - loss: 0.5179 - val_loss: 0.3650

Epoch 00001: val_loss improved from inf to 0.36503, saving model to model.hdf5
Epoch 2/1000
100/100 [==============================] - 151s 2s/step - loss: 0.2034 - val_loss: 0.1344

Epoch 00002: val_loss improved from 0.36503 to 0.13443, saving model to model.hdf5
Epoch 3/1000
100/100 [==============================] - 152s 2s/step - loss: 0.0757 - val_loss: 0.1152

Epoch 00003: val_loss improved from 0.13443 to 0.11520, saving model to model.hdf5
Epoch 4/1000
100/100 [==============================] - 152s 2s/step - loss: 0.0407 - val_loss: 0.1304

Epoch 00004: val_loss did not improve from 0.11520
Epoch 5/1000
100/100 [==============================] - 152s 2s/step - loss: 0.0281 - val_loss: 0.5231

Epoch 00005: val_loss did not improve from 0.11520
Epoch 6/1000
100/100 [==============================] - 152s 2s/step - loss: 0.0223 - val_loss: 0.0306

Epoch 00006: val_loss improved from 0.11520 to 0.03062, saving model to model.hdf5
Epoch 7/1000
100/100 [==============================] - 152s 2s/step - loss: 0.0185 - val_loss: 0.0397

Epoch 00007: val_loss did not improve from 0.03062
Epoch 8/1000
100/100 [==============================] - 152s 2s/step - loss: 0.0162 - val_loss: 0.0309

Epoch 00008: val_loss did not improve from 0.03062
Epoch 9/1000
100/100 [==============================] - 152s 2s/step - loss: 0.0163 - val_loss: 0.0298

Epoch 00009: val_loss improved from 0.03062 to 0.02977, saving model to model.hdf5
Epoch 10/1000
100/100 [==============================] - 153s 2s/step - loss: 0.0120 - val_loss: 0.0279

Epoch 00010: val_loss improved from 0.02977 to 0.02790, saving model to model.hdf5
Epoch 11/1000
100/100 [==============================] - 153s 2s/step - loss: 0.0143 - val_loss: 0.0577

Epoch 00011: val_loss did not improve from 0.02790
Epoch 12/1000
100/100 [==============================] - 152s 2s/step - loss: 0.0122 - val_loss: 0.0207

Epoch 00012: val_loss improved from 0.02790 to 0.02070, saving model to model.hdf5
Epoch 13/1000
100/100 [==============================] - 152s 2s/step - loss: 0.0098 - val_loss: 0.0198

Epoch 00013: val_loss improved from 0.02070 to 0.01985, saving model to model.hdf5
Epoch 14/1000
100/100 [==============================] - 152s 2s/step - loss: 0.0130 - val_loss: 0.0184

Epoch 00014: val_loss improved from 0.01985 to 0.01836, saving model to model.hdf5
Epoch 15/1000
100/100 [==============================] - 152s 2s/step - loss: 0.0102 - val_loss: 0.1916

Epoch 00015: val_loss did not improve from 0.01836
Epoch 16/1000
100/100 [==============================] - 152s 2s/step - loss: 0.0107 - val_loss: 0.0149

Epoch 00016: val_loss improved from 0.01836 to 0.01494, saving model to model.hdf5
Epoch 17/1000
100/100 [==============================] - 153s 2s/step - loss: 0.0102 - val_loss: 0.0253

Epoch 00017: val_loss did not improve from 0.01494
Epoch 18/1000
100/100 [==============================] - 152s 2s/step - loss: 0.0085 - val_loss: 0.0323

Epoch 00018: val_loss did not improve from 0.01494
